<!--
  ~ Smart Data Lake Examples - Build your data lake the smart way.
  ~
  ~ Copyright Â© 2019 ELCA Informatique SA (<https://www.elca.ch>)
  ~
  ~ This program is free software: you can redistribute it and/or modify
  ~ it under the terms of the GNU General Public License as published by
  ~ the Free Software Foundation, either version 3 of the License, or
  ~ (at your option) any later version.
  ~
  ~ This program is distributed in the hope that it will be useful,
  ~ but WITHOUT ANY WARRANTY; without even the implied warranty of
  ~ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  ~ GNU General Public License for more details.
  ~
  ~ You should have received a copy of the GNU General Public License
  ~ along with this program. If not, see <http://www.gnu.org/licenses/>.
  -->
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<!--
	  By setting sdl-parent as parent you get proper dependency management for Spark and predefined plugin management
	  Alternatively you could import Spark dependency management from sdl-parent with scope=import (or also directly from spark-parent),
	  and need to copy all version properties from sdl-parent.pom into this pom.
	-->
	<parent>
		<groupId>io.smartdatalake</groupId>
		<artifactId>sdl-parent</artifactId>
		<!-- set the smartdatalake version to use here -->
		<version>1.2.0-SNAPSHOT</version>
	</parent>

	<artifactId>sdl-examples</artifactId>

	<name>Smart Data Lake Examples</name>

	<properties>
		<log4j2.version>2.11.2</log4j2.version>
		<scala.deps.scope>provided</scala.deps.scope>
		<spark.deps.scope>provided</spark.deps.scope>
		<hadoop.deps.scope>provided</hadoop.deps.scope>
		<flume.deps.scope>provided</flume.deps.scope>
		<hive.deps.scope>provided</hive.deps.scope>
		<orc.deps.scope>provided</orc.deps.scope>
		<parquet.deps.scope>provided</parquet.deps.scope>
	</properties>

	<build>
		<sourceDirectory>src/main/scala</sourceDirectory>
		<outputDirectory>target</outputDirectory>
		<plugins>
			<!-- Compiles Scala sources. -->
			<plugin>
				<groupId>net.alchim31.maven</groupId>
				<artifactId>scala-maven-plugin</artifactId>
			</plugin>

			<!-- Copies files in resources folders to target folder. -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-resources-plugin</artifactId>
			</plugin>

			<!-- Creates the jar without dependencies -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-jar-plugin</artifactId>
			</plugin>

			<!-- Checks for declared but unused and undeclared but used dependencies in the verify stage -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-dependency-plugin</artifactId>
			</plugin>

			<!-- check for dependency version conflicts -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-enforcer-plugin</artifactId>
			</plugin>

			<!-- Executes examples -->
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>exec-maven-plugin</artifactId>
				<version>1.6.0</version>
				<executions>
					<execution>
						<id>all</id>
						<phase>verify</phase>
						<goals>
							<goal>exec</goal>
						</goals>
						<configuration>
							<arguments>
								<argument>-classpath</argument>
								<classpath/>
								<argument>io.smartdatalake.app.LocalSmartDataLakeBuilder</argument>
								<argument>--feed-sel</argument>
								<argument>.*</argument>
								<argument>--config</argument>
								<argument>${project.build.directory}</argument>
							</arguments>
						</configuration>
					</execution>
				</executions>
				<configuration>
					<executable>java</executable>
					<workingDirectory>${project.build.directory}</workingDirectory>
				</configuration>
			</plugin>
		</plugins>
	</build>

	<dependencies>

		<!-- smartdatalakebuilder without log4j1.2 -->
		<dependency>
			<groupId>io.smartdatalake</groupId>
			<artifactId>sdl-core_${scala.minor.version}</artifactId>
			<version>${project.parent.version}</version>
			<exclusions>
				<exclusion>
					<groupId>log4j</groupId>
					<artifactId>log4j</artifactId>
				</exclusion>
				<exclusion>
					<groupId>log4j</groupId>
					<artifactId>apache-log4j-extras</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>slf4j-log4j12</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<!-- log4j2 -->
		<dependency>
			<groupId>org.apache.logging.log4j</groupId>
			<artifactId>log4j-core</artifactId>
			<version>${log4j2.version}</version>
		</dependency>
		<dependency>
			<groupId>org.apache.logging.log4j</groupId>
			<artifactId>log4j-api-scala_${scala.minor.version}</artifactId>
			<version>11.0</version>
			<exclusions>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-api</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.apache.logging.log4j</groupId>
			<artifactId>log4j-slf4j-impl</artifactId>
			<version>${log4j2.version}</version>
			<exclusions>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-core</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<!-- route log4j1.2 over over slf4j to log4j2 -->
		<dependency>
			<groupId>org.slf4j</groupId>
			<artifactId>log4j-over-slf4j</artifactId>
			<version>1.7.25</version>
		</dependency>

		<!-- override scope for spark dependencies to include/exclude them correctly for the fat-jar -->
		<!-- note that the corresponding profile defining spark.deps.scope is inherited from sdl-parent -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>${spark.deps.scope}</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-sql_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>${spark.deps.scope}</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-hive_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>${spark.deps.scope}</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-catalyst_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>${spark.deps.scope}</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-avro_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>${spark.deps.scope}</scope>
		</dependency>

	</dependencies>

</project>
