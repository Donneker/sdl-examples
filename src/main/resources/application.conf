#
# Smart Data Lake - Build your data lake the smart way.
#
# Copyright Â© 2019 ELCA Informatique SA (<https://www.elca.ch>)
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.
#

global {
  sparkOptions {
    "hive.exec.dynamic.partition" = true
    "hive.exec.dynamic.partition.mode" = nonstrict
    "spark.sql.sources.partitionOverwriteMode" = dynamic
    "spark.shuffle.service.enabled" = true
    "spark.rdd.compress" = true
    "spark.checkpoint.compress" = true

    // sizing, should normally be set by spark-submit parameters
    //"spark.executor.cores": "4",
    //"spark.executor.memory": "8G",
    //"spark.yarn.executor.memoryOverhead": "2048"

    // dynamic allocation
    //"spark.dynamicAllocation.enabled" = true
    //"spark.dynamicAllocation.minExecutors" = 0
    //"spark.dynamicAllocation.maxExecutors" = 15,
    //"spark.dynamicAllocation.executorIdleTimeout" = 300s,
    //"spark.dynamicAllocation.cachedExecutorIdleTimeout" = 1h,

    // others
    //"spark.sql.parquet.writeLegacyFormat" = true // to write parquet in legacy format for hive compatibility
    //"spark.default.parallelism" = 200 // should normally be set per action with property parallelism (to be implemented)
    //"spark.files.maxPartitionBytes" = 134217728 // default is 128Mb, that's a good value
  }
}

connections {

  localHsqlJdbc {
    type = JdbcTableConnection
    url = "jdbc:hsqldb:file:csv-to-jdbc/hsqldb"
    driver = org.hsqldb.jdbcDriver
  }

}

dataObjects {

  custom-my-df {
    type = CustomDfDataObject
    creator {
      class-name = io.smartdatalake.samples.CreateMyDf
    }
  }

  custom-csv-entire-room {
    type = CsvFileDataObject
    path = "~{id}/data"
    csv-options {
      delimiter = "|"
      escape = "\\"
      header = "true"
      quote = "\""
    }
  }

  ab-csv-org {
    type = CsvFileDataObject
    path = "AB_NYC_2019.csv"
    csv-options {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
  }

  ab-csv-hadoop {
    type = CsvFileDataObject
    csv-options {
      delimiter = "|"
      escape = "\\"
      header = "true"
      quote = "\""
    }
    path = "~{id}/data"
  }

  ab-parquet-hadoop {
    type = ParquetFileDataObject
    path = "~{id}/data/parquet"
  }

  ab-hive {
    type = HiveTableDataObject
    path = "~{id}/data/hive"
    table {
      db = default
      name = "ab_hive"
      primary-keys = [id]
    }
  }

  ab-excel {
    type = ExcelFileDataObject
    path = "~{id}/AB_NYC_2019.xlsx"
    excel-options {
      sheet-name = csvdata
    }
  }

  ab-reduced-hsqldb {
    type = JdbcTableDataObject
    connectionId = localHsqlJdbc
    pre-sql = "create table if not exists ~{table.db}.~{table.name} (id int, name varchar(255))"
    fail-if-table-missing = false
    table {
      db = public
      name = nyc_reduced
    }
  }

  ab-reduced-csv-hadoop {
    type = CsvFileDataObject
    path = "~{id}/nyc_reduced.csv"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
  }

  custom-rating-df {
    type = CustomDfDataObject
    creator {
      class-name = io.smartdatalake.samples.CreateRatingDf
    }
  }

  custom-rating-csv1 {
    type = CsvFileDataObject
    path = "~{id}.csv"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
  }

  custom-rating-csv2 {
    type = CsvFileDataObject
    path = "~{id}.csv"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
  }

  custom-rating-csv-agg {
    type = CsvFileDataObject
    path = "~{id}.csv"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
  }

  // Export Metadata example
  applications-conf-do {
    type = DataObjectsExporterDataObject
  }

  applications-conf-actions {
    type = ActionsExporterDataObject
  }

  export-data-objects {
    type=CsvFileDataObject
    path = "metadata-data-objects.csv"
    csv-options {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
  }

  export-actions {
    type=CsvFileDataObject
    path = "metadata-actions.csv"
    csv-options {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
  }

}

actions {

  customDf2Csv {
    type = CopyAction
    inputId = custom-my-df
    outputId = custom-csv-entire-room
    metadata {
      feed = custom-my-df2csv
    }
  }

  copyCsv {
    type = CopyAction
    inputId = ab-csv-org
    outputId = ab-csv-hadoop
    metadata {
      feed = ab-csv
    }
  }

  loadParquet {
    type = CopyAction
    inputId = ab-csv-org
    outputId = ab-parquet-hadoop
    metadata {
      feed = ab-parquet-hive
    }
  }

  loadHive {
    type = CopyAction
    inputId = ab-parquet-hadoop
    outputId = ab-hive
    metadata {
      feed = ab-parquet-hive
    }
  }

  loadExcel {
    type = CopyAction
    inputId = ab-csv-org
    outputId = ab-excel
    metadata {
      feed = ab-excel
    }
  }

  loadJdbc {
    type = CopyAction
    inputId = ab-csv-org
    outputId = ab-reduced-hsqldb
    transformer.class-name = io.smartdatalake.samples.ReduceNycCSVTransformer
    metadata {
      feed = ab-jdbc
    }
  }

  loadJdbc2Csv {
    type = CopyAction
    inputId = ab-reduced-hsqldb
    outputId = ab-reduced-csv-hadoop
    metadata {
      feed = ab-jdbc
    }
  }

  customRatingDf2Csv1 {
    type = CopyAction
    inputId = custom-rating-df
    outputId = custom-rating-csv1
    metadata {
      feed = custom-rating-csv
    }
  }
  customRatingDf2Csv2 {
    type = CopyAction
    inputId = custom-rating-df
    outputId = custom-rating-csv2
    metadata {
      feed = custom-rating-csv
    }
  }
  customRatingAgg {
    type = CustomSparkAction
    inputIds = [custom-rating-csv1, custom-rating-csv2]
    outputIds = [custom-rating-csv-agg]
    transformer.class-name = io.smartdatalake.samples.RatingTransformer
    metadata {
      feed = custom-rating-csv
    }
  }

  // export metadata
  export-data-objects {
    type = CopyAction
    inputId = applications-conf-do
    outputId = export-data-objects
    metadata {
      feed = export-metadata
    }
  }

  export-actions {
    type = CopyAction
    inputId = applications-conf-actions
    outputId = export-actions
    metadata {
      feed = export-metadata
    }
  }

}

